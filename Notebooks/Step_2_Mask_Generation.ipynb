{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, glob\n",
    "import gc\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from fast_slic.avx2 import SlicAvx2\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Branching', 'Fish', 'Massive', 'Not Massive', 'Substrate', 'Target', 'Water']\n",
    "\n",
    "labels = {'Branching' : 0, \n",
    "          'Fish' : 1, \n",
    "          'Massive' : 2,\n",
    "          'Not Massive' : 3,\n",
    "          'Substrate' : 4,\n",
    "          'Target' : 5,\n",
    "          'Water' : 6}\n",
    "\n",
    "NO_LABEL = 255\n",
    "N_CLASSES = 7\n",
    "\n",
    "import seaborn as sns\n",
    "cp = sns.color_palette(\"hls\", 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_prediction(pred):\n",
    "   \n",
    "    colored_mask = np.zeros(shape = (pred.shape[0], pred.shape[1], 3))\n",
    "\n",
    "    for _ in np.unique(pred):\n",
    "           \n",
    "            colored_mask[pred == _] = cp[_]\n",
    "        \n",
    "    return colored_mask\n",
    "\n",
    "def display(gt, mask):\n",
    "\n",
    "    plt.figure(figsize = (20, 15))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(gt, alpha = .65)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, alpha = .65)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def compute_segmentations(iterations, start, end):\n",
    "    # change how you calculate the number of segments each iteration\n",
    "    # my method was linear, Shathe's is exponential (much better)\n",
    "    num_iters = iterations\n",
    "    start_sp = start\n",
    "    end_sp = end\n",
    "\n",
    "    reduction_factor = math.pow(float(end_sp) / start_sp, 1. / (num_iters - 1))\n",
    "\n",
    "    return [int(start_sp * math.pow(reduction_factor, iters)) for iters in np.arange(num_iters)]\n",
    "\n",
    "\n",
    "# SciPy's Mode function with an addition to choose the 2nd most common\n",
    "# value in a column (through the 3rd dimension) if the 1st most common\n",
    "# value is 225\n",
    "def mode(a, axis = 0):\n",
    "\n",
    "    a = np.array(a)\n",
    "    scores = np.unique(np.ravel(a))  # all unique values\n",
    "    testshape = list(a.shape)        # dimensions of array\n",
    "    testshape[axis] = 1\n",
    "    oldmostfreq = np.zeros(testshape, dtype = int)    \n",
    "    oldcounts = np.zeros(testshape, dtype = int)       \n",
    "\n",
    "    for score in scores:\n",
    "\n",
    "        if(score == NO_LABEL):\n",
    "            continue\n",
    "\n",
    "        template = (a == score)                                 \n",
    "        counts = np.expand_dims(np.sum(template, axis), axis)\n",
    "\n",
    "        mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)  \n",
    "        oldcounts = np.maximum(counts, oldcounts)\n",
    "        oldmostfreq = mostfrequent\n",
    "\n",
    "    return mostfrequent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_points(file, threshold):\n",
    "        \n",
    "    sparse_points = pd.read_csv(file)   \n",
    "\n",
    "    sparse_points = sparse_points[sparse_points['Confidence'] >= threshold]\n",
    "    \n",
    "    sparse_points = sparse_points.drop('Unnamed: 0', axis = 1)\n",
    "    sparse_points = sparse_points.reset_index(drop = True)\n",
    "\n",
    "    return sparse_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the algorithm that uses SLIC for multiple iterations\n",
    "def get_mask(image, annotation, iterations, start, end, method):\n",
    "    \n",
    "    all_masks = []\n",
    "    segmentations = compute_segmentations(iterations, start, end)\n",
    "\n",
    "    # Loops through, each time a mask is created with different settings, and is accumulated\n",
    "    for _ in range(iterations):\n",
    "        \n",
    "        n_segments = segmentations[_]\n",
    "\n",
    "        # Uses CPU to create segmented image with current params\n",
    "        slic = SlicAvx2(num_components = n_segments, compactness = 20)\n",
    "        segmented_image = slic.iterate(cv2.cvtColor(image, cv2.COLOR_RGB2LAB))\n",
    "\n",
    "        # The XY location of each annotation, along with the label\n",
    "        X = np.array(annotation['X'].values, dtype = 'uint16')\n",
    "        Y = np.array(annotation['Y'].values, dtype = 'uint16')\n",
    "        L = np.array(annotation['Labels'].values, dtype = 'str')  \n",
    "    \n",
    "        # By plugging in the annotation locations into the segmented image\n",
    "        # you get all of the segment IDs that contain annotations (desired segments, DS)\n",
    "        # Then for each DS, find the class label for the annotations within it (color label, CL)\n",
    "        DS = segmented_image[Y, X]                               \n",
    "        CL = np.array([labels.get(L[i]) for i in range(len(DS))])\n",
    "        \n",
    "        # If multiple annotations are within the same segment, find\n",
    "        # the most common label among those annotations and provide it\n",
    "        # as the final label for that segment (majority rule)\n",
    "        if(len(DS) != len(np.unique(DS))):\n",
    "\n",
    "            for segment_ID in np.unique(DS):\n",
    "                annotations_in_segment = list(np.where(DS == segment_ID)[0])\n",
    "                labels_of_annotations = [CL[a] for a in annotations_in_segment]\n",
    "                most_common_label_in = max(set(labels_of_annotations), key = labels_of_annotations.count)\n",
    "                CL[annotations_in_segment] = most_common_label_in\n",
    "        \n",
    "        # Lastly, combine them into a dictionary, which speeds up the the next process by 50%\n",
    "        pairs = dict(zip(DS, CL))\n",
    "\n",
    "        # one mask to hold the labels for each super-pixel (and individual pixels)\n",
    "        mask = np.full(shape = image.shape[0:2], fill_value = NO_LABEL)\n",
    "\n",
    "        for index, segVal in enumerate(list(pairs)):\n",
    "            mask[segmented_image == segVal] = pairs[list(pairs)[index]]\n",
    "            # provides each individual pixel with the class label of the super-pixel\n",
    "\n",
    "        # Collects all masks made of this image for all iterations\n",
    "        all_masks.append(mask)\n",
    "        \n",
    "        gc.collect()\n",
    "                    \n",
    "    # Now that the loop is over, we have a bunch of segmented images where there are\n",
    "    # pixels with class labels, and pixels with no label (255). We can combine them\n",
    "    # following Alonso et al. 2019's join method, or Jordan's mode method\n",
    "    \n",
    "    # Starting with the first mask which is the most detailed, we mask it with less a\n",
    "    # detailed masks that contains more labeled pixels, mask_b. With each iteration, \n",
    "    # the class mask (final_mask) fills up with the additional pixels provided by mask_b; \n",
    "    # no pixels get replaced by mask_b, they only add to final_mask\n",
    "    if(method == 'join'):\n",
    "        # First mask is most detailed\n",
    "        final_mask = all_masks[0]\n",
    "\n",
    "        # Go through the remaining masks\n",
    "        for _ in all_masks[1:]:\n",
    "            mask_b = _\n",
    "\n",
    "            # find the pixels in mask_a that are not labelled, assign them with the labels\n",
    "            # of the pixels in mask_b\n",
    "            final_mask[final_mask == NO_LABEL] = mask_b[final_mask == NO_LABEL] \n",
    "            \n",
    "    # Jordan's method, 'mode'\n",
    "    else:\n",
    "         final_mask = mode(all_masks)\n",
    "            \n",
    "    return final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rates(matrix):\n",
    "\n",
    "    TP = np.diag(matrix);\n",
    "\n",
    "    FP = np.sum(matrix, axis = 0) - TP \n",
    "\n",
    "    FN = np.sum(matrix, axis = 1) - TP \n",
    "\n",
    "    TN = []\n",
    "    for _ in range(len(np.diag(matrix))):\n",
    "        temp = np.delete(matrix, _, 0)    \n",
    "        temp = np.delete(temp, _, 1)  \n",
    "        TN.append(sum(sum(temp)))\n",
    "\n",
    "    return TN, FP, FN, TP\n",
    "\n",
    "def get_scores(ground_truths, predictions):\n",
    "    \n",
    "    flat_ground_truth = []\n",
    "    flat_prediction = []\n",
    "\n",
    "    # Counters variables\n",
    "    correct = np.zeros(N_CLASSES)\n",
    "    actual = np.zeros(N_CLASSES) \n",
    "    predicted = np.zeros(N_CLASSES)\n",
    "    matrix = np.zeros((N_CLASSES, N_CLASSES), np.uint32)\n",
    "\n",
    "    for _ in range(len(predictions)):\n",
    "        \n",
    "        prediction = predictions[_]\n",
    "        ground_truth = ground_truths[_]\n",
    "    \n",
    "        #flat_prediction += prediction.flatten().astype(int).tolist()\n",
    "        ##flat_ground_truth += ground_truth.flatten().astype(int).tolist()\n",
    "\n",
    "        # Computes predicted, correct, real and the confusion matrix per file, accumulates all\n",
    "        for c in range(N_CLASSES):\n",
    "\n",
    "            # Number of predictions per class\n",
    "            predicted[c] = predicted[c] + sum(sum((ground_truth >= 0) & (ground_truth < N_CLASSES) & (prediction == c)))\n",
    "\n",
    "            # Number of correctly predicted samples per class\n",
    "            correct[c] = correct[c] + sum(sum((ground_truth == c ) & (prediction == c)) )   \n",
    "\n",
    "            # Number of real samples per class\n",
    "            actual[c] = actual[c] + sum(sum(ground_truth == c))                   \n",
    "\n",
    "            # Build a confusion matrix\n",
    "            for x in range(N_CLASSES):\n",
    "                matrix[c, x] = matrix[c, x] + sum(sum((ground_truth == c) & (prediction == x)))\n",
    "\n",
    "    # Calculating metrics    \n",
    "    TN, FP, FN, TP = get_rates(matrix)\n",
    "\n",
    "    matrix_normalized = np.around( (matrix / matrix.astype(np.float).sum(axis = 1, keepdims = True)) , 2 )\n",
    "\n",
    "\n",
    "    print(\"Relative Abundance: \")\n",
    "\n",
    "    RA= pd.DataFrame(list(zip(LABELS, np.around(actual/np.sum(actual), 4), np.around(predicted/np.sum(predicted), 4))),\n",
    "                columns= ['Class Labels', 'Ground-Truth', 'Predictions'])\n",
    "    print(RA)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(matrix_normalized)\n",
    "\n",
    "    # Calculates metrics per class (macro), and weights\n",
    "    p = (TP / (TP + FP))\n",
    "    r = (TP / (TP + FN))\n",
    "    dice = (2 * TP) / (TP + FP + FN + TP)\n",
    "    iou = (TP) / (TP + FP + FN)\n",
    "    w = actual/np.sum(actual)\n",
    "\n",
    "    # Overall accuracy using scikit\n",
    "    print(\"\\nOverall Accuracy: \", accuracy_score(flat_ground_truth, flat_prediction))\n",
    "\n",
    "    # Per class and Average Class Accuracy\n",
    "    per_class_accuracy = []\n",
    "    for _ in range(N_CLASSES):\n",
    "        per_class_accuracy.append(correct[_]/actual[_])\n",
    "\n",
    "    print(\"\\nAverage Class Accuracy: \", np.around(np.mean(per_class_accuracy), 4))\n",
    "\n",
    "    print(\"\\nPer Class Accuracy: \")\n",
    "    [print(LABELS[_], \":\", round(per_class_accuracy[_], 3)) for _ in range(N_CLASSES)]\n",
    "\n",
    "    # Macro and weighted metrics\n",
    "    print(\"\\nMacro: \")\n",
    "    print(\"Precision:\", np.around(np.mean(p), 4))\n",
    "    print(\"Recall:\", np.around(np.mean(r), 4))\n",
    "    print(\"Dice:\", np.around(np.mean(dice), 4))\n",
    "    print(\"Iou:\", np.around(np.mean(iou), 4))\n",
    "\n",
    "    print(\"\\nWeighted: \")\n",
    "    print(\"Precision:\", np.around(np.sum([p[_] * w[_] for _ in range(N_CLASSES)]), 4))\n",
    "    print(\"Recall:\", np.around(np.sum([r[_] * w[_] for _ in range(N_CLASSES)]), 4))\n",
    "    print(\"Dice:\", np.around(np.sum([dice[_] * w[_] for _ in range(N_CLASSES)]), 4))\n",
    "    print(\"IoU:\", np.around(np.sum([iou[_] * w[_] for _ in range(N_CLASSES)]), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarking = True\n",
    "\n",
    "if(benchmarking):\n",
    "    path = \"ground_truth\\\\\"\n",
    "else:\n",
    "    path = \"Z:\\\\Photogrammetry_data\\\\Florida\\\\7_16_2019_patch\\\\models\\\\before-algae\\\\Deep Learning\\\\data\\\\\"\n",
    "\n",
    "\n",
    "images = sorted(glob.glob(path + \"images\\\\*.png\"))\n",
    "sparse = sorted(glob.glob(\"Sparse\\\\Manual 100\\\\*.csv\"))\n",
    "gts = sorted(glob.glob(path + \"numpy\\\\*.npy\"))\n",
    "\n",
    "data = pd.DataFrame(data = list(zip(images, gts, sparse)), columns = ['images', 'gts', 'sparse'])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = []\n",
    "predictions = []\n",
    "\n",
    "# Reduction factor: Alonso and I both found that even if you reduce the size of the input image\n",
    "# the results of the dense labels will quantiatiavely be about the same, but Fast-MSS runs even faster\n",
    "# cavet is that by using nearest-neighbor interpolation to upsample the dense labels, it becomes more pixelated\n",
    "rf = 6\n",
    "\n",
    "h, w = 2160//rf, 3840//rf\n",
    "\n",
    "iterations, start, end = 30, 5000, 300\n",
    "\n",
    "for index in range(50):\n",
    "\n",
    "    print(index)\n",
    "\n",
    "    name = data['images'][index].split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    ground_truth = np.argmax(np.load(data['gts'][index]), axis = 2)\n",
    "    image = iaa.Resize({'height' : h, 'width' : w}).augment_image(io.imread(data['images'][index]))\n",
    "\n",
    "    sparse_points = get_sparse_points(data['sparse'][index], threshold = .75)\n",
    "    sparse_points['X'] = sparse_points['X'].values//rf\n",
    "    sparse_points['Y'] = sparse_points['Y'].values//rf\n",
    "\n",
    "    start_time = time.time()\n",
    "    mask = get_mask(image, sparse_points, iterations = iterations, start = start, end = end, method = 'mode')\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    prediction = iaa.Resize({'height' : 2160, 'width' : 3840}, interpolation = 'nearest').augment_image(mask)\n",
    "    #io.imsave(fname = path + \"dense\\\\\" + name + \".png\", arr = mask)\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    ground_truths.append(ground_truth)\n",
    "\n",
    "print(\"Time:\", end_time)\n",
    "get_scores(ground_truths, predictions)\n",
    "\n",
    "print(\"\\n\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
