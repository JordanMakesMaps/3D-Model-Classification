{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import cv2\n",
    "from skimage import io, util\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels, used for assigning classes\n",
    "LABELS = ['Branching', 'Fish', 'Massive', 'Not Massive', 'Substrate', 'Target', 'Water']\n",
    "\n",
    "labels = {'Branching' : 0, \n",
    "          'Fish' : 1, \n",
    "          'Massive' : 2,\n",
    "          'Not Massive' : 3,\n",
    "          'Substrate' : 4,\n",
    "          'Target' : 5,\n",
    "          'Water' : 6}\n",
    "\n",
    "UNIVERSIAL_COLORS = { 'Background' : 0,\n",
    "                      'Branching' : 1, \n",
    "                      'Fish' : 2, \n",
    "                      'Massive' : 3,\n",
    "                      'Not Massive' : 4,\n",
    "                      'Substrate' : 5,\n",
    "                      'Target' : 6,\n",
    "                      'Water' : 7}\n",
    "\n",
    "# The universial value for pixels that do no have a class label\n",
    "NO_LABEL = 255\n",
    "\n",
    "N_CLASSES = len(LABELS)\n",
    "\n",
    "cp = np.array([ [178,24,43],\n",
    "                [239,138,98],\n",
    "                [253,219,199],\n",
    "                [247,247,247],\n",
    "                [209,229,240],\n",
    "                [103,169,207],\n",
    "                [33,102,172] ])\n",
    "\n",
    "\n",
    "labelbox = {\"Background\" : np.array([0, 0, 0]),\n",
    "          \"Massive\" : np.array([117, 219, 87]), \n",
    "          \"Not Massive\" : np.array([87, 219, 170]), \n",
    "          \"Branching\" : np.array([219, 95, 87]) , \n",
    "          \"Fish\" : np.array([219, 208, 87]) ,\n",
    "          \"Substrate\" : np.array([87, 155, 219]) , \n",
    "          \"Target\" : np.array([133, 87, 219]) , \n",
    "          \"Water\" : np.array([219, 87, 192]) } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a binary mask (containing values from 0 - 7 denoting the class), and converts\n",
    "# it into a pretty image using the color palette (cp)\n",
    "def colorize_prediction(pred):\n",
    "   \n",
    "    colored_mask = np.zeros(shape = (pred.shape[0], pred.shape[1], 3))\n",
    "\n",
    "    for _ in range(N_CLASSES):\n",
    "           \n",
    "            colored_mask[pred == _] = list(labelbox)[labelbox[_]]\n",
    "        \n",
    "    return colored_mask\n",
    "\n",
    "# Extracts a square patch with lengths equal to ps * 2 from an image, centered on coordinates x, y\n",
    "def extract_patch(img, x, y):\n",
    "    \n",
    "    patch = img[y - ps : y + ps, x - ps : x + ps]\n",
    "    \n",
    "    return iaa.Resize(224).augment_image(patch) * (1./255.0)\n",
    "\n",
    "\n",
    "\n",
    "# This does the majority of the work in this script:\n",
    "# it is used provide sparse annotations to an image automatically\n",
    "# when provided with an image, it will project X amount of points on\n",
    "# the image, extract patches from the image centered on those points,\n",
    "# and passes all of those patches to the CNN.\n",
    "#\n",
    "# The CNN will make predictions on each patch, and store the label,\n",
    "# the confidence value (how sure the CNN is on its prediction), and \n",
    "# the location (x, y) from the image it was extracted from.\n",
    "# \n",
    "# Input and image\n",
    "# Outputs a dataframe (.csv) with all of the sparse annotations for that image\n",
    "def get_sparse_points(img, percent):\n",
    " \n",
    "    # determines if the points are sampled from a grid, or randomly\n",
    "    # the larger the ratio, the more patches are sampled from grid\n",
    "    # Shoot for about 0.01% of the total number of pixels in image\n",
    "    ratio = 1.0\n",
    "    \n",
    "    num_points = int((img.shape[0] * img.shape[1]) * percent)\n",
    "    density = int(np.sqrt(num_points)) \n",
    "\n",
    "    x_, y_ = np.meshgrid(np.linspace(offset, img.shape[1] - offset, int(density * ratio)), \n",
    "                         np.linspace(offset, img.shape[0] - offset, int(density * ratio)))\n",
    "\n",
    "    xy = np.dstack([x_, y_]).reshape(-1, 2).astype(int)\n",
    "\n",
    "    x = [point[0] for point in xy]\n",
    "    y = [point[1] for point in xy]\n",
    "    \n",
    "    \n",
    "    # If you want all of the points but they don't fit within the grid, \n",
    "    # this will sample the remainder of the randomly\n",
    "    x += np.random.randint(offset, img.shape[1] - offset, num_points - len(xy)).tolist()\n",
    "    y += np.random.randint(offset, img.shape[0] - offset, num_points - len(xy)).tolist()\n",
    "    \n",
    "    patches = np.array([extract_patch(img, point[0], point[1]) for point in list(zip(x, y))])\n",
    "    \n",
    "    predictions = model.predict(patches)\n",
    "    \n",
    "    predicted_labels = [list(labels)[np.argmax(prediction, axis = 0)] for prediction in predictions]\n",
    "    confidence = [sorted(prediction)[-1] - sorted(prediction)[-2] for prediction in predictions]\n",
    "    \n",
    "    sparse_points = pd.DataFrame(list(zip(x, y, predicted_labels, confidence)), \n",
    "                                 columns = ['X', 'Y', 'Labels', 'Confidence'])\n",
    "\n",
    "    \n",
    "    return sparse_points\n",
    "\n",
    "# Works, but there is another metrics function in \n",
    "# Fast-MSS repo that you can use too.\n",
    "def get_sparse_scores(gt, pred):    \n",
    "\n",
    "    cm = confusion_matrix(gt, pred)\n",
    "\n",
    "    accuracy = np.zeros(shape = (N_CLASSES, ))\n",
    "    precision = np.zeros(shape = (N_CLASSES, ))\n",
    "    recall = np.zeros(shape = (N_CLASSES, ))\n",
    "    iou = np.zeros(shape = (N_CLASSES, ))\n",
    "    dice = np.zeros(shape = (N_CLASSES, ))\n",
    "\n",
    "    for i in range(len(np.unique(gt))):\n",
    "\n",
    "        # regions of the confusion matrix (TN)\n",
    "        tl = cm[: i, : i].flatten()\n",
    "        bl = cm[i :, : i].flatten()\n",
    "        tr = cm[: i, i :].flatten()\n",
    "        br = cm[i + 1 :, i + 1 :].flatten()\n",
    "\n",
    "        tp = cm[i][i]\n",
    "        fn = sum(cm[i, np.arange(cm.shape[0]) != i].flatten())\n",
    "        fp = sum(cm[np.arange(cm.shape[0]) != i, i].flatten())\n",
    "        tn = sum(tl) + sum(bl) + sum(tr) + sum(br)\n",
    "        \n",
    "        # to avoid nans since there aren't any instances of the class categories in the samples\n",
    "        if(tp == 0 or tp + fp == 0 or tp + fp + fn == 0):\n",
    "            continue\n",
    "\n",
    "        accuracy[i] = (tp + tn) / (tp + tn + fn + fp) # acc\n",
    "        precision[i] = tp / (tp + fp)\n",
    "        recall[i] = tp / (tp + fn)\n",
    "        iou[i] = tp / (tp + fp + fn)\n",
    "        dice[i] = (2 * precision[i] * recall[i])/(precision[i] + recall[i])\n",
    "    \n",
    "    \n",
    "    return accuracy_score(gt, pred), np.mean(precision), np.mean(recall), np.mean(dice), np.mean(iou)\n",
    "\n",
    "\n",
    "\n",
    "# This determines how accurate the sparse points are against the ground truth (Dense Annotations)\n",
    "# Finds the label in the corresponding pixel-index of the dense annotations, compares against sparse\n",
    "def get_accuracy(gt, sparse_points):\n",
    "     \n",
    "    gt_labels = [gt[sparse_points['Y'][i], sparse_points['X'][i]] for i, r in sparse_points.iterrows()]\n",
    "    gt_labels = [list(labels)[i] for i in gt_labels]\n",
    "    \n",
    "    \n",
    "    return get_sparse_scores(gt_labels, sparse_points['Labels'].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "path = \"ground_truth\\\\\"\n",
    "images = sorted(glob.glob(path + \"Images\\\\*.png\"))\n",
    "gts = sorted(glob.glob(path + \"dense\\\\*.png\"))\n",
    "\n",
    "print(len(gts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a CNN (efficientnetb0) and loads weights made in the image classification script\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.applications.nasnet import NASNetMobile \n",
    "import efficientnet.keras as efn\n",
    "\n",
    "model = Sequential([\n",
    "        efn.EfficientNetB0(weights = 'noisy-student', include_top = False,  pooling = 'max'),\n",
    "        Dropout(.80),\n",
    "        Dense(7),\n",
    "        Activation('softmax')\n",
    "])\n",
    "\n",
    "model.load_weights(\"path_to_labels.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch size (84*2 pixels x 84*2 pixels), keep offset the same as ps, but feel free to change ps\n",
    "# it will do best arround 84 - 112\n",
    "ps = 84\n",
    "offset = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:  0 cam_1_before_1_0\n",
      "cam_1_before_1_0 \n",
      " cam_1_before_1_0\n",
      "2809\n",
      "Time:  14.6\n",
      "image:  1 cam_1_before_1_15\n",
      "cam_1_before_1_15 \n",
      " cam_1_before_1_15\n",
      "2809\n",
      "Time:  14.52\n",
      "image:  2 cam_1_before_1_22\n",
      "cam_1_before_1_22 \n",
      " cam_1_before_1_22\n",
      "2809\n",
      "Time:  14.52\n",
      "image:  3 cam_1_before_1_31\n",
      "cam_1_before_1_31 \n",
      " cam_1_before_1_31\n",
      "2809\n",
      "Time:  14.5\n",
      "image:  4 cam_1_before_1_44\n",
      "cam_1_before_1_44 \n",
      " cam_1_before_1_44\n",
      "2809\n",
      "Time:  14.51\n",
      "image:  5 cam_1_before_1_66\n",
      "cam_1_before_1_66 \n",
      " cam_1_before_1_66\n",
      "2809\n",
      "Time:  14.55\n",
      "image:  6 cam_1_before_1_7\n",
      "cam_1_before_1_7 \n",
      " cam_1_before_1_7\n",
      "2809\n",
      "Time:  14.6\n",
      "image:  7 cam_1_before_1_71\n",
      "cam_1_before_1_71 \n",
      " cam_1_before_1_71\n",
      "2809\n",
      "Time:  14.56\n",
      "image:  8 cam_1_before_1_88\n",
      "cam_1_before_1_88 \n",
      " cam_1_before_1_88\n",
      "2809\n",
      "Time:  14.58\n",
      "image:  9 cam_1_before_1_96\n",
      "cam_1_before_1_96 \n",
      " cam_1_before_1_96\n",
      "2809\n",
      "Time:  14.54\n",
      "image:  10 cam_1_before_2_1041\n",
      "cam_1_before_2_1041 \n",
      " cam_1_before_2_1041\n",
      "2809\n",
      "Time:  14.54\n",
      "image:  11 cam_1_before_2_117\n",
      "cam_1_before_2_117 \n",
      " cam_1_before_2_117\n",
      "2809\n",
      "Time:  14.51\n",
      "image:  12 cam_1_before_2_193\n",
      "cam_1_before_2_193 \n",
      " cam_1_before_2_193\n",
      "2809\n",
      "Time:  14.48\n",
      "image:  13 cam_1_before_2_205\n",
      "cam_1_before_2_205 \n",
      " cam_1_before_2_205\n",
      "2809\n",
      "Time:  14.52\n",
      "image:  14 cam_1_before_2_206\n",
      "cam_1_before_2_206 \n",
      " cam_1_before_2_206\n",
      "2809\n",
      "Time:  14.52\n",
      "image:  15 cam_1_before_2_216\n",
      "cam_1_before_2_216 \n",
      " cam_1_before_2_216\n",
      "2809\n",
      "Time:  14.48\n",
      "image:  16 cam_1_before_2_464\n",
      "cam_1_before_2_464 \n",
      " cam_1_before_2_464\n",
      "2809\n",
      "Time:  14.51\n",
      "image:  17 cam_1_before_2_475\n",
      "cam_1_before_2_475 \n",
      " cam_1_before_2_475\n",
      "2809\n",
      "Time:  14.53\n",
      "image:  18 cam_1_before_2_49\n",
      "cam_1_before_2_49 \n",
      " cam_1_before_2_49\n",
      "2809\n",
      "Time:  14.5\n",
      "image:  19 cam_1_before_2_508\n",
      "cam_1_before_2_508 \n",
      " cam_1_before_2_508\n",
      "2809\n",
      "Time:  14.53\n",
      "image:  20 cam_1_before_2_565\n",
      "cam_1_before_2_565 \n",
      " cam_1_before_2_565\n",
      "2809\n",
      "Time:  14.51\n",
      "image:  21 cam_1_before_2_577\n",
      "cam_1_before_2_577 \n",
      " cam_1_before_2_577\n",
      "2809\n",
      "Time:  14.48\n",
      "image:  22 cam_1_before_2_591\n",
      "cam_1_before_2_591 \n",
      " cam_1_before_2_591\n",
      "2809\n",
      "Time:  14.5\n",
      "image:  23 cam_1_before_2_602\n",
      "cam_1_before_2_602 \n",
      " cam_1_before_2_602\n",
      "2809\n",
      "Time:  14.52\n",
      "image:  24 cam_1_before_2_61\n",
      "cam_1_before_2_61 \n",
      " cam_1_before_2_61\n",
      "2809\n",
      "Time:  14.51\n",
      "image:  25 cam_1_before_2_640\n",
      "cam_1_before_2_640 \n",
      " cam_1_before_2_640\n",
      "2809\n",
      "Time:  14.5\n",
      "image:  26 cam_1_before_2_648\n",
      "cam_1_before_2_648 \n",
      " cam_1_before_2_648\n",
      "2809\n",
      "Time:  14.51\n",
      "image:  27 cam_1_before_2_708\n",
      "cam_1_before_2_708 \n",
      " cam_1_before_2_708\n",
      "2809\n",
      "Time:  14.57\n",
      "image:  28 cam_1_before_2_714\n",
      "cam_1_before_2_714 \n",
      " cam_1_before_2_714\n",
      "2809\n",
      "Time:  14.53\n",
      "image:  29 cam_1_before_2_752\n",
      "cam_1_before_2_752 \n",
      " cam_1_before_2_752\n",
      "2809\n",
      "Time:  14.58\n",
      "image:  30 cam_1_before_2_753\n",
      "cam_1_before_2_753 \n",
      " cam_1_before_2_753\n",
      "2809\n",
      "Time:  14.51\n",
      "image:  31 cam_1_before_2_784\n",
      "cam_1_before_2_784 \n",
      " cam_1_before_2_784\n",
      "2809\n",
      "Time:  14.51\n",
      "image:  32 cam_2_before_1_110\n",
      "cam_2_before_1_110 \n",
      " cam_2_before_1_110\n",
      "2809\n",
      "Time:  14.57\n",
      "image:  33 cam_2_before_1_167\n",
      "cam_2_before_1_167 \n",
      " cam_2_before_1_167\n",
      "2809\n",
      "Time:  14.55\n",
      "image:  34 cam_2_before_1_531\n",
      "cam_2_before_1_531 \n",
      " cam_2_before_1_531\n",
      "2809\n",
      "Time:  14.55\n",
      "image:  35 cam_2_before_1_59\n",
      "cam_2_before_1_59 \n",
      " cam_2_before_1_59\n",
      "2809\n",
      "Time:  14.53\n",
      "image:  36 cam_2_before_1_595\n",
      "cam_2_before_1_595 \n",
      " cam_2_before_1_595\n",
      "2809\n",
      "Time:  14.54\n",
      "image:  37 cam_2_before_1_746\n",
      "cam_2_before_1_746 \n",
      " cam_2_before_1_746\n",
      "2809\n",
      "Time:  14.55\n",
      "image:  38 cam_2_before_1_75\n",
      "cam_2_before_1_75 \n",
      " cam_2_before_1_75\n",
      "2809\n",
      "Time:  14.59\n",
      "image:  39 cam_2_before_1_752\n",
      "cam_2_before_1_752 \n",
      " cam_2_before_1_752\n",
      "2809\n",
      "Time:  14.53\n",
      "image:  40 cam_2_before_1_799\n",
      "cam_2_before_1_799 \n",
      " cam_2_before_1_799\n",
      "2809\n",
      "Time:  14.53\n",
      "image:  41 cam_2_before_1_818\n",
      "cam_2_before_1_818 \n",
      " cam_2_before_1_818\n",
      "2809\n",
      "Time:  14.51\n",
      "image:  42 cam_2_before_2_1316\n",
      "cam_2_before_2_1316 \n",
      " cam_2_before_2_1316\n",
      "2809\n",
      "Time:  14.55\n",
      "image:  43 cam_2_before_2_228\n",
      "cam_2_before_2_228 \n",
      " cam_2_before_2_228\n",
      "2809\n",
      "Time:  14.55\n",
      "image:  44 cam_2_before_2_607\n",
      "cam_2_before_2_607 \n",
      " cam_2_before_2_607\n",
      "2809\n",
      "Time:  14.57\n",
      "image:  45 cam_2_before_2_644\n",
      "cam_2_before_2_644 \n",
      " cam_2_before_2_644\n",
      "2809\n",
      "Time:  14.55\n",
      "image:  46 cam_2_before_2_687\n",
      "cam_2_before_2_687 \n",
      " cam_2_before_2_687\n",
      "2809\n",
      "Time:  14.61\n",
      "image:  47 cam_2_before_2_814\n",
      "cam_2_before_2_814 \n",
      " cam_2_before_2_814\n",
      "2809\n",
      "Time:  14.56\n",
      "image:  48 cam_2_before_2_868\n",
      "cam_2_before_2_868 \n",
      " cam_2_before_2_868\n",
      "2809\n",
      "Time:  14.59\n",
      "image:  49 cam_2_before_2_924\n",
      "cam_2_before_2_924 \n",
      " cam_2_before_2_924\n",
      "2809\n",
      "Time:  14.58\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "\n",
    "# loops through each of the images, gets sparse points for each, compares the sparse points with the corresponding gt\n",
    "# the scores should be about 90%. The goal is to be higher, but 90% is a good baseline.\n",
    "for index in range(50):\n",
    "   \n",
    "    basename = images[index].split(\"\\\\\")[-1].split(\".\")[0]; print(\"image: \", str(index), basename)\n",
    "    \n",
    "    print(images[index].split(\"\\\\\")[-1].split(\".\")[0], \"\\n\", images[index].split(\"\\\\\")[-1].split(\".\")[0])\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    gt = io.imread(gts[index])\n",
    "    image = io.imread(images[index])\n",
    "        \n",
    "    # percent is the number of pixels in the image given annoations\n",
    "    sparse_points = get_sparse_points(image, percent = .00035); num_points = len(sparse_points); \n",
    "    \n",
    "    sparse_points.to_csv(\"Sparse\\\\Manual 75\\\\\" + basename + \".csv\")\n",
    "    print(len(sparse_points))\n",
    "\n",
    "    #score = get_accuracy(gt, sparse_points); scores.append(score)\n",
    "\n",
    "    \n",
    "    print(\"Time: \", round(time() - start, 2))\n",
    "    \n",
    "\n",
    "#print(\"Average Classification Accuracy:\", np.mean(scores, axis = 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just viewing the last image\n",
    "sparse_ = sparse_points[sparse_points['Confidence'] >= .70]\n",
    "X = sparse_.X.values\n",
    "Y = sparse_.Y.values\n",
    "C = [labelbox[_]/255.0 for _ in sparse_.Labels.values]\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(image, alpha = .85)\n",
    "plt.scatter(X, Y, c = C)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
