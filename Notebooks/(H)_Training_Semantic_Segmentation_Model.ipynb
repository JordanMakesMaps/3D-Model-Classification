{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "This notebook is meant to demonstrate how to train Fully Convolutional Network (for more tutorials please see Qubvel's [repo](https://github.com/qubvel/segmentation_models). \n",
    "\n",
    "\"Although the dense labels created by Fast-MSS could have been used to classify the 3-D reconstructed model directly, they were also used as training data with a deep learning semantic segmentation algorithm to produce a FCN. The major advantage of a FCN is its ability to generalize to images collected from domains that are similar to those on which it was trained. A researcher could obtain dense labels from an FCN given images collected from the same or similar habitats that it was previously trained on without having to perform any of the previous steps in the workflow (steps B-G). Thus, the objective of this workflow was not just to obtain a set of dense labels for every still image, but rather to acquire a deep learning semantic segmentation model that could create dense labels automatically for datasets collected in the future.\n",
    "\n",
    "This study experimented with five different FCNs to understand how the size of the network affected the classification accuracy. Each FCN used an encoder from the EfficientNet series (Tan and Le, 2019) and was used to create an additional set of dense labels for every image in the dataset; these and the set created by Fast-MSS were validated and compared against the ground-truth dense labels that were manually created for the test set.\"\n",
    "\n",
    "...\n",
    "\n",
    "\"For the task of semantic segmentation this study experimented with five different FCNs, all of which used the U-Net architecture and were equipped with one of the five smallest encoders within the EfficientNet family (i.e., B0 through B4, see Supplementary Information 4 for more information). All models were implemented in Python using the Segmentation Models library (Yakubovskiy, 2019).\n",
    "\n",
    "When training the FCNs, the error was calculated using the soft-Jaccard loss function, which acted as a differentiable proxy that attempted to maximize the Intersection-over-Union metric (Berman et al., 2018). Parameters were updated via backpropagation using the Adam optimizer with an initial learning rate of 10â€“4, which decreased using the same settings as described before. After 20 epochs, the weights from the epoch with the lowest validation loss were archived. All deep learning models were trained on a PC equipped with a NVIDIA GTX 1080 Ti GPU and an Intel i7-8700 CPU, using the Keras deep learning framework and the Tensorflow numerical computational library; for more information see Supplementary Information 4.\"\n",
    "\n",
    "![alt text](../Figures/getting_dense_labels.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import segmentation_models as sm\n",
    "from segmentation_models.losses import *\n",
    "from segmentation_models.metrics import *\n",
    "sm.set_framework('keras')\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "keras.backend.set_image_data_format('channels_last')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "import imgaug.augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for each class category of interest used for Pierce et al., 2021\n",
    "\n",
    "class_categories = {'Branching' : 0, \n",
    "                      'Fish' : 1, \n",
    "                      'Massive' : 2,\n",
    "                      'Not Massive' : 3,\n",
    "                      'Substrate' : 4,\n",
    "                      'Target' : 5,\n",
    "                      'Water' : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data\\\\\"\n",
    "\n",
    "images = sorted(glob.glob(path + \"images\\\\*.png\"))\n",
    "masks = sorted(glob.glob(path + \"dense\\\\*.png\"))\n",
    "points = sorted(glob.glob(path + \"sparse\\\\*.csv\"))\n",
    "\n",
    "data = pd.DataFrame(list(zip(images, masks, points)), columns = ['Images', 'Masks', 'Points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(data, test_size = .1)\n",
    "\n",
    "train.reset_index(drop = True, inplace = True)\n",
    "valid.reset_index(drop = True, inplace = True)\n",
    "\n",
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_prediction(pred):\n",
    "   \n",
    "    colored_mask = np.zeros(shape = (pred.shape[0], pred.shape[1], 3))\n",
    "\n",
    "    for _ in np.unique(pred):\n",
    "           \n",
    "            colored_mask[pred == _] = cp[_]/255.0\n",
    "        \n",
    "    return colored_mask\n",
    "\n",
    "\n",
    "# Image data generator class\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, dataframe, batch_size, augment, n_classes = 8):\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.augment = augment\n",
    "          \n",
    "        \n",
    "    # Steps per epoch    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) // self.batch_size\n",
    "    \n",
    "    # Shuffles and resets the index at the end of training epoch\n",
    "    def on_epoch_end(self):\n",
    "        self.dataframe = self.dataframe.reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    # Generates data, feeds to training\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        global preprocess_input\n",
    "        \n",
    "        processed_images = []\n",
    "        processed_masks = []\n",
    "        \n",
    "        for _ in range(self.batch_size):\n",
    "\n",
    "            the_image = plt.imread(self.dataframe['Images'][index])\n",
    "            the_mask = plt.imread(self.dataframe['Masks'][index]).astype('uint8');\n",
    "            one_hot_mask = to_categorical(the_mask, len(list(class_categories)))\n",
    "            \n",
    "            if(self.augment):\n",
    "                \n",
    "                processed_image = augs_for_images(image = the_image)\n",
    "                processed_mask = augs_for_masks(image = one_hot_mask)\n",
    "         \n",
    "            else:\n",
    "                # Still resizing and then random cropping, but no augmentations   \n",
    "                processed_image = resize_for_images(image = the_image)\n",
    "                processed_mask = resize_for_masks(image = one_hot_mask)\n",
    "\n",
    "            processed_images.append(preprocess_input(processed_image))\n",
    "            processed_masks.append(processed_mask)\n",
    "\n",
    "                \n",
    "        batch_x = np.array( processed_images )\n",
    "        batch_y = np.array( processed_masks )\n",
    "        \n",
    "        return (batch_x, batch_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for training      \n",
    "batch_size = 1\n",
    "num_epochs = 100\n",
    "\n",
    "steps_per_epoch_train = len(train) // batch_size; print(steps_per_epoch_train)\n",
    "steps_per_epoch_valid = len(valid) // batch_size; print(steps_per_epoch_valid)\n",
    "\n",
    "train_gen = DataGenerator(train, batch_size = batch_size, augment = True) \n",
    "valid_gen = DataGenerator(valid, batch_size = batch_size, augment = False)\n",
    "\n",
    "height, width = 736, 1280 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation methods\n",
    "augs_for_images = iaa.Sequential([iaa.Resize(size = {'height' : height, 'width' : width}, interpolation = 'linear',\n",
    "                                            random_state = 5),\n",
    "                                  iaa.Fliplr(0.25, random_state = 1),\n",
    "                                  iaa.Flipud(0.25, random_state = 2),\n",
    "                                  iaa.Rot90([1, 2, 3, 4], True, random_state = 3)\n",
    "                       ])\n",
    "\n",
    "\n",
    "augs_for_masks = iaa.Sequential([iaa.Resize(size = {'height' : height, 'width' : width}, interpolation = 'nearest',\n",
    "                                           random_state = 5),\n",
    "                                  iaa.Fliplr(0.25, random_state = 1),\n",
    "                                  iaa.Flipud(0.25, random_state = 2),\n",
    "                                  iaa.Rot90([1, 2, 3, 4], True, random_state = 3)\n",
    "                                ])\n",
    "\n",
    "\n",
    "\n",
    "resize_for_images = iaa.Sequential([\n",
    "     iaa.Resize(size = {'height' : height, 'width' : width}, interpolation = 'linear', random_state = 1),\n",
    "])\n",
    "\n",
    "resize_for_masks = iaa.Sequential([\n",
    "     iaa.Resize(size = {'height' : height, 'width' : width}, interpolation = 'nearest', random_state = 1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'efficientnetb0'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE) \n",
    "\n",
    "model = sm.Unet(input_shape = (None, None, 3), \n",
    "                backbone_name = BACKBONE, \n",
    "                encoder_weights = 'noisy-student', # 'imagenet'\n",
    "                activation = 'softmax', \n",
    "                classes = len(list(class_categories)),\n",
    "                encoder_freeze = True,\n",
    "                decoder_use_batchnorm = True)\n",
    "\n",
    "\n",
    "metrics = ['accuracy', iou_score, precision, recall]\n",
    "\n",
    "model.compile(optimizer = Adam(lr = .001), \n",
    "              loss = [cce_jaccard_loss], \n",
    "              metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"weights\\\\\", exist_ok=False) \n",
    "\n",
    "hollabackgirl = [\n",
    "                 ReduceLROnPlateau(monitor = 'val_loss', factor = .65, patience = 2, verbose = 1),\n",
    "                 ModelCheckpoint(filepath = 'weights\\\\model-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', \n",
    "                                 monitor='val_loss', save_weights_only = True, \n",
    "                                 save_best_only = True, verbose = 1),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator = train_gen, \n",
    "                              steps_per_epoch = steps_per_epoch_train, \n",
    "                              epochs = num_epochs, \n",
    "                              validation_data = valid_gen,\n",
    "                              validation_steps = steps_per_epoch_valid,\n",
    "                              verbose = 1,\n",
    "                              callbacks = holla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "plt.figure(figsize= (10, 5))\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.argmin(history.history[\"val_loss\"]), \n",
    "         np.min(history.history[\"val_loss\"]), \n",
    "         marker = \"x\", color = \"b\", label = \"best model\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize= (10, 5))\n",
    "plt.plot(history.history[\"precision\"], label=\"precision\")\n",
    "plt.plot(history.history[\"val_precision\"], label=\"val_precision\")\n",
    "plt.title(\"Training Precision\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize= (10, 5))\n",
    "plt.plot(history.history[\"recall\"], label=\"recall\")\n",
    "plt.plot(history.history[\"val_recall\"], label=\"val_recall\")\n",
    "plt.title(\"Training Recall\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights\\\\path_to_best_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions with the trained model\n",
    "\n",
    "for _ in range(5):\n",
    "    \n",
    "    image, mask = valid_gen.__getitem__(_)\n",
    "    prediction = model.predict(image)\n",
    "    prediction = np.argmax(prediction, axis = 2).astype(\"uint8\")\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(colorize_prediction(mask))\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(colorize_prediction(prediction))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
