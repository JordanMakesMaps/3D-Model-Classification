{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# labels for each class category of interest\n",
    "LABELS = ['Branching', 'Fish', 'Massive', 'Not Massive', 'Substrate', 'Target', 'Water']\n",
    "\n",
    "labels = {'Branching' : 0, \n",
    "          'Fish' : 1, \n",
    "          'Massive' : 2,\n",
    "          'Not Massive' : 3,\n",
    "          'Substrate' : 4,\n",
    "          'Target' : 5,\n",
    "          'Water' : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects all of the patches, change as needed\n",
    "data = glob.glob(\"Patches\\\\Manual\\\\**\\\\*.bmp\", recursive = False)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.choice(data, size = int(len(data) * .25), replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes all of the data and splits it into training, validation and test sets\n",
    "# provides the correponsding labels for each patch based on the folder it is located in\n",
    "training_files, validation_files = train_test_split(data, test_size = .1)\n",
    "validation_files, test_files = train_test_split(validation_files, test_size = .1)\n",
    "\n",
    "training_labels = [file.split(\"\\\\\")[-2] for file in training_files]\n",
    "validation_labels = [file.split(\"\\\\\")[-2] for file in validation_files]\n",
    "test_labels = [file.split(\"\\\\\")[-2] for file in test_files]\n",
    "\n",
    "\n",
    "train = pd.DataFrame(data = list(zip(training_files, training_labels)), columns = ['images', 'labels'])\n",
    "valid = pd.DataFrame(data = list(zip(validation_files, validation_labels)), columns = ['images', 'labels'])\n",
    "test = pd.DataFrame(data = list(zip(test_files, test_labels)), columns = ['images', 'labels'])\n",
    "\n",
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# Augmentation methods\n",
    "augs_for_train = iaa.Sequential([   iaa.Resize(224, interpolation = 'linear'),\n",
    "                          iaa.Fliplr(0.5),\n",
    "                          iaa.Flipud(0.5),\n",
    "                          iaa.Rot90([1, 2, 3, 4], True),\n",
    "                          iaa.Sometimes(.3, iaa.Affine(scale = (.95, 1.05))),\n",
    "                          iaa.Sometimes(.1, iaa.Invert(1.0)),\n",
    "                          iaa.Sometimes(.5, iaa.SomeOf((0, 1), \n",
    "                                             [\n",
    "                                                 iaa.MedianBlur(3),\n",
    "                                                 iaa.ChannelShuffle(.7),\n",
    "                                                 iaa.EdgeDetect(.5)\n",
    "                                             ])),\n",
    "\n",
    "                          iaa.Sometimes(.5, iaa.SomeOf((0, 1),\n",
    "                                            [\n",
    "                                                 iaa.Dropout(.2),\n",
    "                                                 iaa.ImpulseNoise(.2),\n",
    "                                                 iaa.SaltAndPepper(.2)\n",
    "                                            ]))\n",
    "                       ])\n",
    "\n",
    "\n",
    "augs_for_valid = iaa.Sequential([iaa.Resize(224, interpolation = 'linear')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data generators to take the files in the dataframes previously created, and creates a pipeline\n",
    "# Patches are augmented and rescaled, and then during training, validation, testing are fed directly\n",
    "# to the model\n",
    "#\n",
    "# Batch size is dependent on the amount of memory available on your machine\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Training images are augmented, and then lightly pre-processed\n",
    "train_augmentor = ImageDataGenerator(preprocessing_function = augs_for_train.augment_image,\n",
    "                                     rescale = 1.0/255.0)\n",
    "                                     \n",
    "                                                                   \n",
    "# Reading from dataframe, can save augmented images if needed\n",
    "train_generator = train_augmentor.flow_from_dataframe(dataframe = train, directory = None,\n",
    "                                                      x_col = 'images', y_col = 'labels', target_size = (224, 224), \n",
    "                                                      color_mode = \"rgb\",  class_mode = 'categorical', \n",
    "                                                      batch_size = batch_size, shuffle = True, seed = 42)\n",
    "                                                     \n",
    "\n",
    "\n",
    "# Only pre-process images, no augmentation\n",
    "validate_augmentor = ImageDataGenerator( preprocessing_function = augs_for_valid.augment_image,\n",
    "                                         rescale = 1.0/255.0 )\n",
    "\n",
    "# Reading from dataframe                             \n",
    "validation_generator = validate_augmentor.flow_from_dataframe(dataframe = valid, directory = None, \n",
    "                                                              x_col = 'images', y_col = 'labels', target_size = (224, 224), \n",
    "                                                              color_mode = \"rgb\",  class_mode = 'categorical', \n",
    "                                                              batch_size = batch_size, shuffle = True, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the length of an epoch, all images used\n",
    "steps_per_epoch_train = len(train)/batch_size\n",
    "\n",
    "# Defines the length of an epoch, all images used\n",
    "steps_per_epoch_valid = len(valid)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the model, starts with noise-student weights\n",
    "# find the efficentnet repo here:\n",
    "# https://github.com/qubvel/efficientnet\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "import efficientnet.keras as efn \n",
    "\n",
    "model = Sequential([\n",
    "        efn.EfficientNetB0(weights = 'noisy-student', include_top = False,  pooling = 'max'),\n",
    "        Dropout(.80),\n",
    "        Dense(7),\n",
    "        Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some metrics\n",
    "from keras import optimizers, losses, metrics\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some callbacks, learning rate will reduce every 2 epochs by * .65\n",
    "# if the validation loss does not decrease. Only the weights from the epoch with\n",
    "# the lowest validation loss will be saved.\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "holla = [\n",
    "         ReduceLROnPlateau(monitor = 'val_loss', factor = .65, patience = 2, verbose = 1),\n",
    "         ModelCheckpoint(filepath = 'path_to_weights.h5', \n",
    "                         monitor='val_loss', save_weights_only = True, \n",
    "                         save_best_only = True, verbose = 1),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the loss function, optimizier and metrics, probably don't need to change\n",
    "# except maybe the learing rate \n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = optimizers.Adam(lr = .0001), \n",
    "              metrics=['acc', precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traing the model, logs the results of the training in history\n",
    "history = model.fit_generator(train_generator, \n",
    "                              steps_per_epoch = steps_per_epoch_train, \n",
    "                              epochs = 100, \n",
    "                              validation_data = validation_generator, \n",
    "                              validation_steps = steps_per_epoch_valid,\n",
    "                              callbacks = holla,\n",
    "                              verbose = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, loads the best weights\n",
    "model.load_weights('path_to_labels.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads from dataframe for test set\n",
    "test_generator = validate_augmentor.flow_from_dataframe(dataframe = test, \n",
    "                                                 x_col = 'images', y_col = 'labels', target_size = (224, 224), \n",
    "                                                 color_mode = \"rgb\",  class_mode = 'categorical', \n",
    "                                                 batch_size = batch_size, shuffle = False, seed = 42)\n",
    "# Defines the length of an epoch\n",
    "steps_per_epoch_test = len(test)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provides a confusion matrix of the results\n",
    "# classification accuracy should be above 97%\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Results, stores predictions for thresholding, shuffling needs to stay off for test\n",
    "predictions = model.predict_generator(test_generator, steps = steps_per_epoch_test)\n",
    "predict_classes = np.argmax(predictions, axis = 1)\n",
    "\n",
    "test_y = test_generator.classes\n",
    "print(\"# of images:\", len(predict_classes))\n",
    "print(accuracy_score(y_true = test_y, y_pred = predict_classes))\n",
    "print(confusion_matrix(y_true = test_y, y_pred = predict_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher values represents more sure/confident predictions\n",
    "# .1 unsure -> .5 pretty sure -> .9 very sure\n",
    "\n",
    "# Look at creating a graph of the threshold values and the accuracy\n",
    "# useful for determing how sure the model is when making predictions\n",
    "\n",
    "threshold_values = np.arange(0.0, 1.0, 0.05)\n",
    "class_ACC = []\n",
    "\n",
    "for threshold in threshold_values:\n",
    "    sure_index = []\n",
    "\n",
    "    for i in range(0, len(predictions)):\n",
    "        if( (sorted(predictions[i])[-1]) - (sorted(predictions[i])[-2]) > threshold):\n",
    "            sure_index.append(i)\n",
    "\n",
    "    sure_test_y = np.take(test_y, sure_index, axis = 0)\n",
    "    sure_pred_y = np.take(predict_classes, sure_index)\n",
    "\n",
    "    class_ACC.append(accuracy_score(sure_test_y, sure_pred_y)) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(threshold_values, class_ACC)\n",
    "plt.xlabel('Threshold Values')\n",
    "plt.xlim([0, 1])\n",
    "plt.xticks(ticks = np.arange(0, 1.05, 0.1))\n",
    "plt.ylabel('Classification Accuracy')\n",
    "plt.title('Identifying the ideal threshold value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
